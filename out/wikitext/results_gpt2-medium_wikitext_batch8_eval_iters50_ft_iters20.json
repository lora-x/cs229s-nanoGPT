{
    "config": {
        "DEBUG": false,
        "master_process": false,
        "out_dir": "out/wikitext",
        "eval_interval": 10,
        "log_interval": 10,
        "eval_iters": 50,
        "eval_only": false,
        "always_save_checkpoint": false,
        "init_from": "gpt2-medium",
        "wandb_log": false,
        "wandb_project": "wikitext",
        "wandb_run_name": "gpt2-medium_wikitext_batch8_eval_iters50_ft_iters20-1702836797.0187747",
        "dataset": "wikitext",
        "gradient_accumulation_steps": 40,
        "batch_size": 8,
        "block_size": 1024,
        "n_layer": 12,
        "n_head": 12,
        "n_embd": 768,
        "dropout": 0.1,
        "bias": false,
        "learning_rate": 0.0006,
        "max_iters": 20,
        "weight_decay": 0.1,
        "beta1": 0.9,
        "beta2": 0.95,
        "grad_clip": 1.0,
        "decay_lr": true,
        "warmup_iters": 2000,
        "lr_decay_iters": 600000,
        "min_lr": 6e-05,
        "backend": "nccl",
        "device": "cuda",
        "dtype": "bfloat16",
        "compile": false
    },
    "tokens_per_iter": 327680,
    "losses": {
        "0": {
            "train": 4.033341884613037,
            "val": 4.038511276245117
        },
        "10": {
            "train": 4.011224746704102,
            "val": 3.9967522621154785
        },
        "50": null,
        "100": null
    },
    "best_val_loss": 1000000000.0,
    "average_time_per_iter": 19.821295380592346,
    "max_memory_per_gpu": 15548109260.8
}