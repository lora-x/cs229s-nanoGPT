{
    "config": {
        "DEBUG": false,
        "master_process": true,
        "out_dir": "out/shakespeare",
        "eval_interval": 10,
        "log_interval": 10,
        "eval_iters": 50,
        "eval_only": false,
        "always_save_checkpoint": false,
        "init_from": "gpt2-medium",
        "wandb_log": true,
        "wandb_project": "shakespeare",
        "wandb_run_name": "gpt2-medium_shakespeare_batch8_eval_iters50_ft_iters20-1702831070.2639117",
        "dataset": "shakespeare",
        "gradient_accumulation_steps": 40,
        "batch_size": 8,
        "block_size": 1024,
        "n_layer": 12,
        "n_head": 12,
        "n_embd": 768,
        "dropout": 0.1,
        "bias": false,
        "learning_rate": 3e-05,
        "max_iters": 20,
        "weight_decay": 0.1,
        "beta1": 0.9,
        "beta2": 0.95,
        "grad_clip": 1.0,
        "decay_lr": false,
        "warmup_iters": 2000,
        "lr_decay_iters": 600000,
        "min_lr": 6e-05,
        "backend": "nccl",
        "device": "cuda",
        "dtype": "bfloat16",
        "compile": false
    },
    "tokens_per_iter": 327680,
    "losses": {
        "0": {
            "train": 4.146543502807617,
            "val": 3.9814319610595703
        },
        "10": {
            "train": 3.794405460357666,
            "val": 3.5859673023223877
        },
        "50": null,
        "100": null
    },
    "best_val_loss": 3.554321527481079,
    "average_time_per_iter": 14.680779838562012,
    "max_memory_per_gpu": 15548576588.8
}